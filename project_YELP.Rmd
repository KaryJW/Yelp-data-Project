---
title: "Yelp Data Project"
author: "Jiawei Li"
date: "2017/11/18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#1.Introduction

##1.Problem Statements
There are many Beauty and Spas businesses in United states. The purpose of my project is try to find out what factors will have impacts on the rating of the beauty shops. Will the location impact the rating of the shops? Are there have any difference between shops that have longer opening hours and have shorter opening hours? If the Beauty shops provide many different items or products such as nails spa, hair salon, medical spas and so on, will the shops get higher rating? What about the influence from the number of the reviews that those shops have? 

There might have other factors that will affect the ratings, but the questions above may be the main questions that I want to figure out in my project.


##2. Data
Yelp challenge dataset:  business, hours, category, attribute, checkin.


a.The business data contains 156639 observations of 12 variables.


b.The hours data contains 734421 observations of 2 variables. 


c.The category data contains590290 observation of 2 variables


d.The attribute data contains 156639 observations of 12 variables


e.The ckeckin data contains 156639 observations of 12 variables

#2.Data Cleaning 
First we need to connect to the database and select the dataset we need (business/hours/category). After Listing those data into a table and checking their columns, I retrieved the data from MySQL and started to clean the data.


After getting data from MySQL, I select the beauty shops in the category dataset. Based on the Yelp data description of beauty shops, I filter all the categories that are related to the beauty shops, and then make it as a dataframe. And also calculate the number of category for each shops. 

What's more, as the original dataset of hours is not in the formation that I want, I do some manipulation to get the information about the how many days each shop opens in a week and how long do the shops open in a day(Insted of using the specific time in different day, I use the average working time here, since the working time varies at different days).

Moreover, I calculate the total number of checkin per shop and pick five variables taht may have impacts on the stars from attribute dataset. To clean the attribute dataset, I first select the beauty shops and then change the form of the dataframe. There are a lot of variables in the attribute dataset, but some of these variables contain too much NA values. So I pick up 5 variables that have enough values (that means that the variables have at least 10,000 values in the attribute dataset after transformation[14,692 values]) and may have impact on the stars of the shops. These five variables are BikeParking, BusinessAcceptsCreditCards, ByAppointmentOnly, RestaurantsPriceRange2,number_Pway. 

At last, I eliminate those shops that are not open, merge three dataframe together and filter out the shops in the US. 


```{r, message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
library(data.table)
library(RMySQL)
library(foreign)
library(dplyr)
library(stringr)
library(tidyr)
library(ggmap)
library(maps)
library(lme4)
library(arm)
library(MASS)
library(VGAM)
library(kableExtra)
library(sjPlot)
```

```{r, include=FALSE}
mydb = dbConnect(MySQL(), user='mssp', password='mssp2017', dbname='yelp_db', host='45.63.90.29')
#Return a list of the tables in our connection. 
dbListTables(mydb) 
dbListFields(mydb, 'business') 
dbListFields(mydb, 'category') 
dbListFields(mydb, 'hours') 
dbListFields(mydb, 'attribute')
dbListFields(mydb, 'checkin')

business.sql = dbSendQuery(mydb, "select * from business ")  # still in mysql
business = fetch(business.sql, n = -1)   # fetch back to R
dbClearResult(business.sql)
catergory.sql = dbSendQuery(mydb, "select * from category ")  
category = fetch(catergory.sql, n = -1)   
dbClearResult(catergory.sql)
hours.sql = dbSendQuery(mydb, "select * from hours ")  
hours = fetch(hours.sql, n = -1)   
dbClearResult(hours.sql)
attribute.sql = dbSendQuery(mydb, "select * from attribute ")  
attribute = fetch(attribute.sql, n = -1)   
dbClearResult(attribute.sql)
checkin.sql = dbSendQuery(mydb, "select * from checkin ")  
checkin = fetch(checkin.sql, n = -1)   
dbClearResult(checkin.sql)

```





```{r, message=FALSE, warning=FALSE, include=FALSE}
#1.
category2 <- filter(category, category %in% c("Beauty & Spas","Acne Treatment",
                                              "Cosmetics & Beauty Supply","Day Spas",
                                              "Erotic Massage","Eyebrow Services",
                                              "Eyelash Service","Teeth Whitening","Tanning",
                                              "Tattoo","Skin Care","Piercing",
                                              "Permanent Makeup","Perfume","Nail Salons",
                                              "Medical Spas","Massage","Makeup Artists",
                                              "Hot Springs","Hair Salons","Hair Extensions",
                                              "Hair Loss Centers","Hair Removal",
                                              "Laser Hair Removal","Sugaring",
                                              "Threading Services","Waxing","Blow Dry/Out Services",
                                              "Hair Extensions","Hair Stylist","Men's Hair Salons",
                                              "Nail Technicians","Spray Tanning",
                                              "Tanning Beds","Barbers"))

category3<-data.frame(table(category2$business_id))
names(category3)[1]<-paste("business_id")
names(category3)[2]<-paste("numbers_category")

#2.
hours2 <-separate(hours, hours, c("w","start","t","endt","t2"))
hours3 <-dplyr::select(hours2, w, start, endt, business_id)

hours3 <-tbl_df(hours3)
hours3$endt <- as.numeric(hours3$endt)
hours3$start <- as.numeric(hours3$start)
hours3$worktime <- abs(hours3$endt-hours3$start)
hours4 <- dplyr::select(hours3, worktime, business_id)
options(digits=3)
hours4 <- hours4 %>% group_by(business_id) %>% summarise(mean(worktime))
names(hours4)[2]<-paste("avg_worktime")
hours4 <- hours4[-which(hours4$avg_worktime == 0), ]

hours5<-data.frame(table(hours3$business_id))
names(hours5)[1]<-paste("business_id")
names(hours5)[2]<-paste("workday")
hours6 <- merge(hours4,hours5,by=c("business_id"))


#3.
checkin2 <- summarise(group_by(checkin,business_id),sum=sum(count))
names(checkin2)[2]<-paste("sumcheckin")

#4.
cate.attribute <- merge(category3,attribute,by=c("business_id"))
cate.attribute2 <- spread(cate.attribute,name,value = value)
colSums(is.na(cate.attribute2))
# Select the columns that have at least 10,000 entries are not NA.
cate.attribute3 <-dplyr::select(cate.attribute2, business_id, numbers_category, BikeParking, BusinessAcceptsCreditCards,BusinessParking,ByAppointmentOnly, RestaurantsPriceRange2)
cate.attribute3<-separate(cate.attribute3, BusinessParking, c("na","garage","one","street","two","validated","three","lot","four","valet","five"))
parking <- dplyr::select(cate.attribute3, business_id,one,two,three,four,five)
parking$one <-ifelse(parking$one=="true",1,0)
parking$two <-ifelse(parking$two=="true",1,0)
parking$three <-ifelse(parking$three=="true",1,0)
parking$four <-ifelse(parking$four=="true",1,0)
parking$five <-ifelse(parking$five=="true",1,0)
parking$number_Pway <- parking$one+parking$two+parking$three+parking$four+parking$five

cate.attribute4 <- merge(cate.attribute3,parking,by=c("business_id"))
cate.attribute4 <- dplyr::select(cate.attribute4, business_id,business_id, numbers_category, BikeParking, BusinessAcceptsCreditCards,ByAppointmentOnly, RestaurantsPriceRange2,number_Pway)

cate.attribute4 <- na.omit(cate.attribute4)
names(cate.attribute4)[6]<-paste("Pricerange")
cate.attr.hour <-  merge(hours6,cate.attribute4,by=c("business_id"))
ccah <- merge(cate.attr.hour,checkin2,by=c("business_id"))

#5.
business1 <- business[-which(business$is_open == 0), ]
total <- merge(ccah,business1,by.x="business_id",by.y = "id")
total <- filter(total, state %in% c("AL", "AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL",
                             "IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE",
                             "NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD",
                             "TN","TX","UT","VT","VA","WA","WV","WI","WY")) 
dim(total)
write.csv(total, file="Beauty_Shops_data.csv")
```


{"garage": true, "street": false, "validated": false, "lot": false, "valet": false}


#3.Exploratory Data Analysis
In this part, I would like to find out how many states have beauty shops records in yelp first, and then try to figure out the variables that may have impact on the stars of the shops.
I find that there are only 9 state have the records of beauty shops in the yelp.And graph two shows the number of beauty shops in each state. AZ has the most higher number of beauty shops and AL has the  most lower number of beauty shops.[see Appendix.1.(1)]

Let's look at the histogram of stars and the boxplot of stars that group by states.
The histogram of stars shows that the count of shops increase with the increase of the ranking. 

```{r, echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}

hist(total$stars,main="Figure.1 Histogram of Stars",xlab = "Stars",ylab = "Frequency")
#The average stars in different state.#Boxplot of the stars, group by state.

```



By comparing the stars in different state in Figure.2, we can see that the mean of different states are within 4-5 stars. Since in AL, there is only 1 observation, so the mean is equal to the value of stars in that observation. We can see from the plot that these boxplot is comparatively tall, which means that people hold quite different opinions in rating the beauty shops. Besides, the long lower whisker of these states  means that stars are varied amongst the most least positive quartile group. What's more, there are some outliers in some states.

 
```{r,echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}

 ggplot(total,aes(state,stars))+geom_boxplot(fill="slateblue1",colour="slateblue4")+
  ggtitle("Figure.2 Comparison among States-Stars")

```


After that, we can look at the distribution of average worktime/ workday/ number of category / the review count.


```{r, echo=FALSE, fig.height=4, fig.width=6}
gridExtra::grid.arrange(
  ggplot(total)+geom_boxplot(aes(state,avg_worktime),fill="slateblue1",colour="slateblue4")+
    ggtitle("Figure.3 Comparison among States-Worktime"),
  ggplot(total)+geom_boxplot(aes(state,workday),fill="steelblue",colour="steelblue4")+
    ggtitle("Figure.4 Comparison among States-Workday"),
  ggplot(total)+geom_boxplot(aes(state,numbers_category),fill="slategray3",colour="slategray4")+
    ggtitle("Figure.5 Comparison among States-# Category"),
  ggplot(total)+geom_boxplot(aes(state,review_count),fill="steelblue",colour="royalblue")+
    ggtitle("Figure.6 Comparison among States-# Review"),
  ncol=2)


```

We can see a common problems from the plot that each variables contains many outlier, especially the review_count plot. In the worktime graph, we can see that the mean of average worktime in different staes
are quite similar, and the box plot is comparatively short. In the workday plot, we can see that some states have long lower whisker, which means that workdays are varied amongst the most least positive quartile group. In the numbers of category plot, we can see that the mean values are different in different states. In the review plot, there might are not showing an obvious patterns. 


Let's explore more information about the variables and its impacts in the stars by the plot below. Figure.7 shows the relations between review counts and stars. Most of the beauty shops have little review counts. In NV and AZ, it shows that more review counts may result in higher stars. In Figure.8 , comparing the average worktime to 10, it seems that those shops with longer worktime will get a lower stars, especially in state AZ and WI. Figure.9 and Figure.10 are not clearly enough to show the relations, but these plots show that workday and the number of category may have some impacts on the stars. As a result, we need to fit the model in order to make a more clearly and reasonable conclusion.


```{r, echo=FALSE, fig.height=8, fig.width=10}
gridExtra::grid.arrange(
ggplot(total) + aes(x=review_count, y=stars, col=factor(state)) + geom_jitter() + facet_grid(.~state)+ggtitle("Figure.7 The Relations Between Review_counts and Stars"),
ggplot(total) + aes(x=avg_worktime, y=stars, col=factor(state)) + geom_jitter() +geom_vline(xintercept=10)+facet_grid(.~state)+ggtitle("Figure.8 The Relations Between Avg-worktime and Stars"),
ggplot(total) + aes(x=workday, y=stars, col=factor(state)) + geom_jitter() + facet_grid(.~state)+ggtitle("Figure.9 The Relations Between Workday and Stars"),
ggplot(total) + aes(x=numbers_category, y=stars, col=factor(state)) + geom_jitter() + facet_grid(.~state)+ggtitle("Figure.10 The Relations Between Workday and Stars"),
ncol=2)
```

Besides those plots above, I also make other plots that contains other variables' information. For example, we can see from the plot of bike parking (Figure.3)that most of the high ranking beauty shops provides bike parking services. What's more, look at the density plot of Appointmentonly Data(Figure.5), it shows that most shops with 5 stars needs an appointment. [see Appendix.1.(2)] As these variables are not the major ones that I want to explore, so I put these plot in the Appendix.



# 4. Model Fitting
The goal of the project is to figure out what variables would have impact on the stars in yelp.
We should define the varibles first.
 -Outcome variable: stars.
 -Predictors: Average worktime in a week, Workday, Review counts, Numbers of category, BikeParking, BusinessAcceptsCreditCards, ByAppointmentOnly, PriceRange, number_Pway.
 -Group: State.
 

Since the review counts varies a lot in different shops, I do a linear tranformation of it(Centering by subtracting the mean of the data). And since state AL has only one observation, I change AZ as the baseline of the model.
As the outcome variable ordinal catergory variables, I use multinomial logistic regression to fit the model.First, I put part of the predict variables into the model.(I choose Average worktime in a week, Workday, Review counts, Numbers of category as variables in the first model.)

```{r, echo=TRUE, message=FALSE, warning=FALSE}
total$review_count <- total$review_count-mean(total$review_count)
total <- within(total, state <- relevel(factor(state), ref = "AZ"))

fit1 <-polr(ordered(stars) ~ review_count+numbers_category+avg_worktime+workday+factor(state), 
            data=total)
#display(fit1)
fit2 <-polr(ordered(stars) ~ review_count+numbers_category+avg_worktime+workday+factor(state)+
              factor(BikeParking)+factor(BusinessAcceptsCreditCards)+factor(Pricerange)+
              factor(ByAppointmentOnly)+number_Pway+sumcheckin, data=total)
#display(fit2)
fit3 <- lm(stars ~ review_count+numbers_category+avg_worktime+workday+factor(state),
           data = total)
#summary(fit3)
fit4 <-lm(stars ~ review_count+numbers_category+avg_worktime+workday+factor(state)+
            factor(BikeParking)+factor(BusinessAcceptsCreditCards)+factor(Pricerange)+
            factor(ByAppointmentOnly)+number_Pway+sumcheckin, data=total)
#summary(fit4)

```


The coeffients of Fit 1 model have been shown below. For review counts (and other continuous variables), the interpretation is that when review counts moves 1 unit, the odds of moving from "1"" applying to "1.5" or "1.5" applying (or from the lower and middle categories to the high category) are multiplied by exp(0.001). What's more, we can find that, only review counts,number of category and state AL and NV have positive impacts on the stars. Others are have negative impacts on stars. When looking at the linear model, it is the same as multinomial model that only review counts,number of category and state AL and NV have positive impacts on the stars. Others are have negative impacts on stars.In the linear model, we can find that one unit of review counts changes can result in 0.001 changes of the satrs.One unit of the number of category can result in 0.060 changes in the stars. Other variables' impacts on the stars can be found in the table below.


```{r, echo=FALSE}
knitr::kable(as.data.frame(round(coef(fit1, 5), digit=4)),caption = "Coefficients of Multinomial Model")
predy <- predict(fit1,data=data.frame(total),type="prob")
knitr::kable(as.data.frame(round(coef(fit3, 5), digit=4)),caption = "Coefficients of Linear Model")
knitr::kable(as.data.frame(AIC(fit1,fit2,fit3,fit4)),caption = "AIC Values Table")
table(pred=predict(fit1),obs=total$stars)
#res1<- model.matrix(~factor(stars)-1,data=total)-fitted(fit1) 
#table(pred=round(predict(fit3)),obs=total$stars)
```


After fitting the multinomial model with variables that I am interested in, I found that the model are not fitting well. Then I add some other variables that may help explain the variations and fit a new model(fit2) . Besides, I fit the linear model with 5 variables (fit3) and 10 variables (fit4) to compare the difference between multinomial model and linear model.  

By comparing the AIC of the model, I find that with the increase of predicted variables, the variations of model can be reduced. What's more, it is surprising that linear models are fitted better than the multinomial logistic model.

The predicted values of multinomial model(fit1) are shown in the last table. It seems that the preicted values are not really similar to the observed values.

##Mixed Effect Model
In this part, I want to figure out the random effect in different state with different review_count, so I make a plot of review count, group by different state.


```{r, echo=FALSE,fig.height=4,fig.width=6}
ggplot(total)+geom_point()+aes(x=stars,y=review_count)+
  facet_wrap(~state)+geom_smooth(method="lm",se=FALSE)+ggtitle("Figure.11 The Plot of Random Effect in Each State with Review Count")
```

From the plot above, we see little random effect on each states with difference of review count. So I decide to find the random effect of intercept and fit a model with random intercept.

```{r, message=FALSE, warning=FALSE,fig.width=5,fig.height=4}
mod1 <- lmer(stars ~ review_count+numbers_category+avg_worktime+workday+(1|state),data=total )
knitr::kable(as.data.frame(fixef(mod1)),caption = "Fixed Effect of Model")
sjp.lmer(mod1, y.offset = .4,title = "Figure.12 Random Effect of Model")
```

The fixed effect shows that review counts and category counts have positive effect on the stars, average worktime and workday have negative effect on the stars. In addition, according to the plot above, we can see that shops in  SC,NV,IL,AZ,Al are getting higher stars than other states. 

#5.Conclusion

In this project, I fit four regression model and a mixed effect model. According to the result of the multinomial regression, I find that review counts,number of category and state AL and NV have positive impacts on the stars while others are not. What's more, By increasing predict variables in the same model, we can have better understanding of the variations of the model. Moreover, it is surprising that using simple linear regressionis better to fit the data than using a multinomial regression. Besides, considering the random effect of different states, the beauty shops in SC,NV,IL,AZ,Al are getting higher stars than those in WI,PA,OH and NC. So there have a random effect in each states. The limitations of my project is model and predict variables' selection. Thus, future studies could focus on these two direction to improve the outcome. 



 
# Appendix
##1. Exploratory Data Analysis
###(1).Distribution about the 
```{r, echo=FALSE, fig.height=4, fig.width=6, warning=FALSE}
#The distribution of the beauty shops in the US.
us <-map_data('state')
ggplot()+
  geom_polygon(data = us,aes(x=long,y=lat,group=group),color='rosybrown1',fill='salmon',alpha=.7)+
  geom_point(aes(x=longitude,y=latitude),size=.7,color='salmon4',data = total)+
  xlim(-125,-65)+ylim(23,50)+labs(title="Figure.1 Distribution of Shops in US")
#The number of the beauty shops and the average stars in each state.
```

```{r, echo=FALSE, fig.height=4, fig.width=6, warning=FALSE}
ggplot(data.frame(total),aes(x=state))+geom_bar(fill = "tomato",alpha=0.8,colour= "tomato4")+
  geom_text(stat='count',aes(label=..count..),vjust=-1)+labs(title="Figure.2 Number of Shops in each State")

```

###(2) Plot of other variables

```{r, echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}


ggplot(total, aes(x=stars,fill=BikeParking)) + geom_bar(position = "fill",width=0.3)+
  facet_wrap(~state)+ggtitle("Figure.3 The Plot of BikeParking")+scale_fill_brewer(palette="Reds")


```


```{r, echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(total, aes(x=stars,fill=BusinessAcceptsCreditCards)) + 
  geom_bar(position = "fill",width=0.3)+
  facet_wrap(~state)+ggtitle("Figure.4 The Plot of BusinessAcceptsCreditCards")+
  scale_fill_brewer(palette="Oranges")
```



```{r,echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(total, aes(x=stars, color=ByAppointmentOnly)) + 
  geom_density() + 
  facet_wrap(~state)+ggtitle("Figure.5 The Plot of Appointmentonly")
```


```{r,echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(total, aes(x=stars,col=factor(number_Pway))) + 
  geom_density() + 
  facet_wrap(~state)+ggtitle("Figure.6 The Plot of Number of Parking Way")

```



```{r,echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(total, aes(x=stars, color=Pricerange)) + 
  geom_density() + 
  facet_wrap(~state)+ggtitle("Figure.7 The Plot of Pricerange")

```





```{r,echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(total, aes(x=stars, y=sumcheckin,col=factor(state))) + 
  geom_jitter() + 
  facet_wrap(~state)+ggtitle("Figure.8 The Plot of Checkin")

```



